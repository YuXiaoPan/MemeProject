{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/home/dluser/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/dluser/MemeProject/im2txt/memes/y-u-no.jpg', '/home/dluser/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194)\n",
      "(100, 16802, 16802)\n",
      "sizing error\n",
      "(200, 35976, 35976)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 55027, 55027)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 74039, 74039)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 92598, 92598)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 111071, 111071)\n",
      "sizing error\n",
      "(700, 129455, 129455)\n",
      "sizing error\n",
      "(800, 147576, 147576)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 165608, 165608)\n",
      "(1000, 183722, 183722)\n",
      "sizing error\n",
      "(1100, 201838, 201838)\n",
      "(1200, 218644, 218644)\n",
      "sizing error\n",
      "(1300, 236360, 236360)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 252935, 252935)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 269052, 269052)\n",
      "(1600, 285491, 285491)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 301117, 301117)\n",
      "sizing error\n",
      "(1800, 315906, 315906)\n",
      "sizing error\n",
      "(1900, 330982, 330982)\n",
      "(2000, 346091, 346091)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 361910, 361910)\n",
      "(2200, 375182, 375182)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389491, 389491)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 402469, 402469)\n",
      "(2500, 414398, 414398)\n"
     ]
    }
   ],
   "source": [
    "with open('/home/dluser/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/home/dluser/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        #match = [s.split('-',1)[-1].lstrip() for s in captions if meme_name in s]\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        \n",
    "        while meme_name in captions[counter]:\n",
    "                if counter==len(captions)-1:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    break\n",
    "                elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385708"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = list(zip(data_memes, data_captions))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled = zip(*no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"haaaaayyyy we're fabulousssssss\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_shuffled[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385708\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))\n",
    "#word_captions = list(set(word_captions))\n",
    "#print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150280)\n",
      "('Words in vocabulary:', 41153)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38520\n",
      "('Wrote vocabulary file:', 'vocab3.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 2\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary\n",
    "with tf.gfile.FastGFile('vocab3.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44847  , -0.039212 ,  0.10393  , -0.18311  , -0.22794  ,\n",
       "        0.071043 ,  0.68074  , -1.0702   ,  0.10879  ,  0.32581  ,\n",
       "       -0.0075187, -0.7147   ,  0.38525  , -0.035042 ,  0.084247 ,\n",
       "        0.21304  ,  0.025578 , -0.38933  , -0.43553  , -0.17755  ,\n",
       "        0.4037   , -0.10913  , -0.056811 , -0.044236 , -0.12161  ,\n",
       "       -0.37139  , -0.3668   , -0.78518  ,  0.1406   ,  0.41383  ,\n",
       "       -0.06213  ,  0.64063  , -0.43111  , -0.16398  ,  0.15145  ,\n",
       "        0.51573  ,  0.39917  , -0.4754   ,  0.24798  , -0.13286  ,\n",
       "        0.30063  , -0.077097 , -0.064587 ,  0.30917  ,  0.65839  ,\n",
       "        0.0059184,  0.10049  ,  0.34199  ,  0.16988  ,  0.15465  ,\n",
       "        0.2108   , -0.16124  ,  0.16009  , -0.058738 ,  0.0602   ,\n",
       "       -0.4676   , -0.41398  ,  0.13282  ,  0.34347  ,  0.31409  ,\n",
       "       -0.2003   , -0.18261  ,  0.28005  ,  0.37784  , -0.085211 ,\n",
       "       -0.82782  , -0.19752  , -0.16085  , -0.051853 , -0.046978 ,\n",
       "       -0.090662 , -0.50244  ,  0.24386  ,  0.29688  , -0.2001   ,\n",
       "        0.19445  , -0.28373  , -0.18313  , -0.39333  ,  0.065854 ,\n",
       "       -0.011839 , -0.23055  , -0.43664  ,  0.080693 , -0.26789  ,\n",
       "       -0.3679   , -0.35613  ,  0.056707 , -0.20576  ,  0.17426  ,\n",
       "       -0.22824  ,  0.15239  , -0.056002 , -0.0097033, -0.30139  ,\n",
       "        0.14433  ,  0.40497  ,  0.054102 ,  0.68279  ,  0.14556  ,\n",
       "        0.0012223,  0.35829  , -0.1578   , -0.26101  , -0.19565  ,\n",
       "        0.10731  ,  0.049116 , -0.25991  ,  0.20451  ,  0.37388  ,\n",
       "        0.091763 ,  0.01326  , -0.16377  , -0.058506 , -0.21469  ,\n",
       "        0.060703 ,  0.16069  , -0.59625  , -0.039471 , -0.17519  ,\n",
       "        0.17627  , -0.35163  , -0.13807  ,  0.18959  , -0.33912  ,\n",
       "       -0.017737 ,  0.2403   ,  0.48794  ,  0.090305 ,  0.24129  ,\n",
       "        0.1004   ,  0.14941  , -0.040279 , -0.23966  , -0.010672 ,\n",
       "       -0.14114  ,  0.25642  , -0.29952  , -0.15572  , -0.66248  ,\n",
       "        0.17192  , -0.20204  ,  0.24225  ,  0.05396  ,  0.08926  ,\n",
       "        0.12152  , -0.3615   ,  0.15428  ,  0.27732  , -0.089112 ,\n",
       "        0.31128  , -0.14316  ,  0.18377  , -0.10577  ,  0.5478   ,\n",
       "        0.12911  ,  0.47722  , -0.50255  ,  0.12407  , -0.060889 ,\n",
       "        0.11583  ,  0.22377  , -0.39128  , -0.097678 , -0.16674  ,\n",
       "       -0.50881  ,  0.27624  , -0.39481  ,  0.17492  ,  0.63578  ,\n",
       "       -0.15305  ,  0.16731  , -0.030143 ,  0.35387  ,  0.18817  ,\n",
       "       -0.52209  , -0.13349  , -0.2518   , -0.51798  ,  0.62682  ,\n",
       "        0.060017 , -0.041401 , -0.028021 ,  0.11233  ,  0.11447  ,\n",
       "        0.44617  , -0.032094 , -0.052696 , -0.038342 ,  0.3574   ,\n",
       "        0.30677  , -0.51606  ,  0.16839  , -0.024237 ,  0.22719  ,\n",
       "        0.018605 ,  0.45249  ,  0.26825  ,  0.263    ,  0.40592  ,\n",
       "       -0.44148  , -0.060587 , -0.25607  , -0.44626  , -0.10527  ,\n",
       "       -0.66607  ,  0.48883  , -0.54111  ,  0.1403   , -0.0476   ,\n",
       "       -0.20509  , -0.36993  ,  0.15421  , -0.093329 ,  0.4347   ,\n",
       "        0.14337  ,  0.052247 , -0.17844  , -0.1292   , -0.18123  ,\n",
       "        0.15523  , -0.14887  , -0.058583 ,  0.40293  ,  1.0223   ,\n",
       "        0.0631   , -0.28373  , -0.14486  , -0.014079 , -0.4358   ,\n",
       "       -0.46028  , -0.70722  ,  0.69638  , -0.035604 , -0.18943  ,\n",
       "       -0.30317  ,  0.080057 , -0.21485  ,  0.10918  ,  0.28817  ,\n",
       "       -0.07454  , -0.079632 ,  0.452    ,  0.011182 ,  0.4781   ,\n",
       "       -0.25833  ,  0.34898  , -0.016316 ,  0.092757 , -0.37827  ,\n",
       "       -0.27047  , -0.03206  ,  0.30495  , -0.18112  , -0.023923 ,\n",
       "       -0.46283  ,  0.40218  ,  0.25496  ,  0.03829  , -0.16211  ,\n",
       "       -0.029873 , -0.033979 ,  0.16065  , -0.26032  , -0.24203  ,\n",
       "        0.12691  , -0.31384  ,  0.48393  , -0.73027  , -0.24479  ,\n",
       "       -0.30343  ,  0.19906  ,  0.12204  ,  0.38072  ,  0.509    ,\n",
       "        0.15137  , -0.52911  ,  0.065863 , -0.3758   , -0.1274   ,\n",
       "        0.080856 ,  0.11545  ,  0.40141  , -0.21268  ,  0.71381  ,\n",
       "        0.1793   ,  0.13791  , -0.37751  ,  0.24224  , -0.12526  ,\n",
       "        0.24988  , -0.39193  , -0.23447  ,  0.38872  ,  0.96019  ,\n",
       "       -0.033372 , -0.08502  , -0.374    , -0.44978  ,  0.80095  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embedding_matrix4',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38518, 38517, 38517, 3, 1588, 67, 1, 155, 35, 38519]\n",
      "formulation clinique. fuck me, right?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38517"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8813\n",
      "74329\n",
      "81974\n",
      "90628\n",
      "114107\n",
      "118722\n",
      "175009\n",
      "298768\n",
      "336991\n",
      "338745\n",
      "365180\n",
      "383446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31996\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353700"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memes_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0  1747968554496765                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  5719692230224609    56565985083580\n",
      "                 0                 0   308499872684478                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0  5824480533599854\n",
      "                 0                 0   528273284435272                 0\n",
      "                 0                 0                 0                 0\n",
      " 11019714355468750                 0  7669129848480225  1160511255264282\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0  5286480426788330                 0 10362536430358886\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      " 13893063545227050                 0                 0                 0\n",
      "                 0  1466020464897155  1875388622283935                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0   264784127473831  1286320924758911\n",
      "                 0                 0                 0                 0\n",
      " 11113889694213868 13664608955383300                 0   720333695411682]\n"
     ]
    }
   ],
   "source": [
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'point blank... 100% pake g-cash\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(captions_shuffled[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/353700\n",
      "Train data: 20000/353700\n",
      "Train data: 40000/353700\n",
      "Train data: 60000/353700\n",
      "Train data: 80000/353700\n",
      "Train data: 100000/353700\n",
      "Train data: 120000/353700\n",
      "Train data: 140000/353700\n",
      "Train data: 160000/353700\n",
      "Train data: 180000/353700\n",
      "Train data: 200000/353700\n",
      "Train data: 220000/353700\n",
      "Train data: 240000/353700\n",
      "Train data: 260000/353700\n",
      "Train data: 280000/353700\n",
      "Train data: 300000/353700\n",
      "Train data: 320000/353700\n",
      "Train data: 340000/353700\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "train_filename = 'train.tfrecords6'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()),  #this is the part that needs to be a float save\n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for capt in captions_shuffled:\n",
    "    if 'what do we say to' in capt:\n",
    "        #print(capt)\n",
    "        num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(enumerate(['a','b','c','d','e']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'a'), (2, 'c')]\n"
     ]
    }
   ],
   "source": [
    "for i,wp in enumerate(x):\n",
    "    if wp[0] == 1:\n",
    "        del x[i]\n",
    "x = x[0:2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile('vocab2.txt', mode=\"r\") as f:\n",
    "      wordidx_pairs = list(f.readlines())\n",
    "wordidx_pairs = [(line.split()[0],line.split()[1]) for line in wordidx_pairs]\n",
    "vocab = dict([(x, int(y)) for (x, y) in wordidx_pairs]) #changed this to reflect vocab.txt format\n",
    "x = sorted(vocab.iteritems(), key=lambda x: int(x[1]))\n",
    "reverse_vocab = [y[0] for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
